{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üîç Smart Document Search Engine\n",
                "\n",
                "## üìö Overview\n",
                "This notebook implements a **TF-IDF (Term Frequency-Inverse Document Frequency)** search engine from scratch. \n",
                "It supports both **Arabic** and **English** documents and uses **Cosine Similarity** to rank search results.\n",
                "\n",
                "### üöÄ Features\n",
                "- **Bilingual Support**: Handles English and Arabic text.\n",
                "- **PDF Processing**: Extracts text from PDF files automatically.\n",
                "- **Vector Space Model**: Represents text as mathematical vectors.\n",
                "- **Ranked Retrieval**: Returns the most relevant results first.\n",
                "\n",
                "---\n",
                "\n",
                "## üõ†Ô∏è Step 1: Setup Workspace\n",
                "First, we install the necessary libraries."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages\n",
                "!pip install flask nltk PyPDF2"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üì¶ Step 2: Imports & Initialization\n",
                "We import standard libraries for file handling and math, along with `nltk` for natural language processing."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "import math\n",
                "import os\n",
                "import glob\n",
                "from collections import defaultdict\n",
                "\n",
                "# PDF Handling\n",
                "import PyPDF2\n",
                "\n",
                "# NLP Libraries\n",
                "import nltk\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.stem import WordNetLemmatizer\n",
                "\n",
                "# Download necessary NLTK datasets\n",
                "print(\"‚è≥ Downloading NLTK resources...\")\n",
                "nltk.download('stopwords', quiet=True)\n",
                "nltk.download('wordnet', quiet=True)\n",
                "print(\"‚úÖ NLTK resources ready.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## üî§ Step 3: Text Preprocessing\n",
                "\n",
                "The quality of a search engine depends heavily on preprocessing. We perform:\n",
                "1.  **Normalization**: Unifying character forms (especially for Arabic).\n",
                "2.  **Tokenization**: Splitting text into words.\n",
                "3.  **Stopword Removal**: Removing common words like \"the\", \"in\", \"ŸÅŸä\", \"ŸÖŸÜ\".\n",
                "4.  **Lemmatization**: Converting words to their base form (e.g., \"running\" -> \"run\")."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==========================================\n",
                "# Configuration & Constants\n",
                "# ==========================================\n",
                "\n",
                "# Define Arabic Stopwords manually as they might be incomplete in NLTK\n",
                "ARABIC_STOPWORDS = {\n",
                "    \"ŸÅŸä\", \"ÿπŸÑŸâ\", \"ŸÖŸÜ\", \"ÿ•ŸÑŸâ\", \"ÿπŸÜ\", \"ŸÖÿπ\", \"Ÿáÿ∞ÿß\", \"Ÿáÿ∞Ÿá\",\n",
                "    \"ŸáŸà\", \"ŸáŸä\", \"ŸáŸÖ\", \"ŸáŸÜ\", \"ŸÉÿßŸÜ\", \"ŸÉÿßŸÜÿ™\", \"ŸäŸÉŸàŸÜ\",\n",
                "    \"ŸÖÿß\", \"ŸÑÿß\", \"ŸÑŸÖ\", \"ŸÑŸÜ\", \"ÿ£ŸÜ\", \"ÿ•ŸÜ\", \"ŸÉŸÑ\", \"ÿ£Ÿä\"\n",
                "}\n",
                "\n",
                "# Initialize English Stopwords and Lemmatizer\n",
                "STOP_WORDS_EN = set(stopwords.words(\"english\"))\n",
                "LEMMATIZER = WordNetLemmatizer()\n",
                "\n",
                "# Regex to identify Arabic characters\n",
                "ARABIC_REGEX = re.compile(r\"[\\u0600-\\u06FF]+\")\n",
                "\n",
                "print(\"‚úÖ Preprocessing configuration loaded.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def normalize_arabic(text: str) -> str:\n",
                "    \"\"\"\n",
                "    Normalize Arabic text by removing diacritics and standardizing characters.\n",
                "    Example: 'ÿ£ÿ≠ŸÖÿØ' -> 'ÿßÿ≠ŸÖÿØ'\n",
                "    \"\"\"\n",
                "    text = re.sub(\"[ÿ•ÿ£ÿ¢ÿß]\", \"ÿß\", text)\n",
                "    text = re.sub(\"Ÿâ\", \"Ÿä\", text)\n",
                "    text = re.sub(\"ÿ§\", \"Ÿà\", text)\n",
                "    text = re.sub(\"ÿ¶\", \"Ÿä\", text)\n",
                "    text = re.sub(\"ÿ©\", \"Ÿá\", text)\n",
                "    text = re.sub(\"[ŸãŸåŸçŸéŸèŸêŸëŸí]\", \"\", text)  # Remove diacritics\n",
                "    return text\n",
                "\n",
                "def is_arabic_token(token: str) -> bool:\n",
                "    \"\"\"Check if the token contains Arabic characters.\"\"\"\n",
                "    return ARABIC_REGEX.search(token) is not None\n",
                "\n",
                "print(\"‚úÖ Arabic normalization functions defined.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def preprocess(text: str) -> list:\n",
                "    \"\"\"\n",
                "    Main preprocessing function.\n",
                "    Input: Raw String\n",
                "    Output: List of cleaned tokens\n",
                "    \"\"\"\n",
                "    if not text:\n",
                "        return []\n",
                "\n",
                "    # 1. Lowercase & Normalize\n",
                "    text = text.lower()\n",
                "    text = normalize_arabic(text)\n",
                "\n",
                "    # 2. Remove Punctuation (keep only word chars and spaces)\n",
                "    text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
                "\n",
                "    tokens = text.split()\n",
                "    clean_tokens = []\n",
                "\n",
                "    for token in tokens:\n",
                "        # Handle Arabic Tokens\n",
                "        if is_arabic_token(token):\n",
                "            if token not in ARABIC_STOPWORDS and len(token) > 1:\n",
                "                clean_tokens.append(token)\n",
                "        \n",
                "        # Handle English Tokens\n",
                "        elif token.isalpha():\n",
                "            if token not in STOP_WORDS_EN:\n",
                "                # Lemmatize: 'running' -> 'run'\n",
                "                clean_tokens.append(LEMMATIZER.lemmatize(token))\n",
                "\n",
                "    return clean_tokens\n",
                "\n",
                "print(\"‚úÖ Main preprocessing function defined.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## üìÑ Step 4: Document Loading (PDFs)\n",
                "\n",
                "We need to extract text from PDF files to build our search index. \n",
                "We split the text into **paragraphs** to make search results more specific (granular)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "MIN_PARAGRAPH_LENGTH = 20\n",
                "\n",
                "def load_pdf(filename: str) -> list:\n",
                "    \"\"\"\n",
                "    Reads a PDF file and returns a list of substantial paragraphs.\n",
                "    \"\"\"\n",
                "    if not os.path.exists(filename):\n",
                "        return []\n",
                "\n",
                "    docs = []\n",
                "    try:\n",
                "        with open(filename, \"rb\") as f:\n",
                "            reader = PyPDF2.PdfReader(f)\n",
                "            for page in reader.pages:\n",
                "                text = page.extract_text()\n",
                "                if not text:\n",
                "                    continue\n",
                "                # Split by newlines to get potential paragraphs\n",
                "                for paragraph in text.split(\"\\n\"):\n",
                "                    paragraph = paragraph.strip()\n",
                "                    if len(paragraph) > MIN_PARAGRAPH_LENGTH:\n",
                "                        docs.append(paragraph)\n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Error loading PDF {filename}: {e}\")\n",
                "        return []\n",
                "    \n",
                "    return docs\n",
                "\n",
                "print(\"‚úÖ PDF loading function defined.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_all_pdfs_from_folder(folder_path: str) -> list:\n",
                "    \"\"\"Scans a directory for all .pdf files and loads them.\"\"\"\n",
                "    all_documents = []\n",
                "    \n",
                "    if not os.path.exists(folder_path):\n",
                "        print(f\"‚ö†Ô∏è Folder not found: {folder_path}\")\n",
                "        return []\n",
                "    \n",
                "    pdf_files = glob.glob(os.path.join(folder_path, \"*.pdf\"))\n",
                "    \n",
                "    if not pdf_files:\n",
                "        print(f\"‚ÑπÔ∏è No PDF files found in: {folder_path}\")\n",
                "        return []\n",
                "    \n",
                "    print(f\"üìÇ Found {len(pdf_files)} PDF file(s). Loading...\")\n",
                "    \n",
                "    for pdf_file in pdf_files:\n",
                "        print(f\"   ‚Üí Processing: {os.path.basename(pdf_file)}\")\n",
                "        docs = load_pdf(pdf_file)\n",
                "        all_documents.extend(docs)\n",
                "    \n",
                "    print(f\"‚úÖ Total loaded documents (paragraphs): {len(all_documents)}\")\n",
                "    return all_documents\n",
                "\n",
                "print(\"‚úÖ Batch PDF loading function defined.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## üßÆ Step 5: The Search Engine Core\n",
                "\n",
                "Here we implement the **Vector Space Model** logic:\n",
                "\n",
                "### 1. TF (Term Frequency)\n",
                "How often a word appears in a specific document.\n",
                "\n",
                "### 2. IDF (Inverse Document Frequency)\n",
                "How unique a word is across all documents. Common words like \"is\" have low IDF, while specific terms like \"algorithm\" have high IDF.\n",
                "\n",
                "$$ IDF(t) = \\log \\left( \\frac{Total\\ Documents}{Documents\\ with\\ term\\ t} \\right) $$\n",
                "\n",
                "### 3. Cosine Similarity\n",
                "Measures the angle between two vectors (Query Vector vs. Document Vector). A value of **1.0** means identical direction (perfect match), **0.0** means no similarity."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_tf(docs: list) -> list:\n",
                "    \"\"\"Calculate Term Frequency (TF) for each document.\"\"\"\n",
                "    tf_docs = []\n",
                "    for doc in docs:\n",
                "        freq = defaultdict(int)\n",
                "        for token in preprocess(doc):\n",
                "            freq[token] += 1\n",
                "        tf_docs.append(freq)\n",
                "    return tf_docs\n",
                "\n",
                "def compute_idf(tf_docs: list) -> dict:\n",
                "    \"\"\"Calculate Inverse Document Frequency (IDF) for all unique terms.\"\"\"\n",
                "    N = len(tf_docs)\n",
                "    df = defaultdict(int)\n",
                "\n",
                "    for doc in tf_docs:\n",
                "        for term in doc.keys():\n",
                "            df[term] += 1\n",
                "\n",
                "    # Standard IDF formula\n",
                "    idf = {term: math.log(N / df_val) for term, df_val in df.items() if df_val > 0}\n",
                "    return idf\n",
                "\n",
                "print(\"‚úÖ TF and IDF calculation functions defined.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def tfidf_vector(tf_doc: dict, idf: dict) -> dict:\n",
                "    \"\"\"\n",
                "    Convert a document's TF dictionary into a TF-IDF vector.\n",
                "    Formula: (1 + log(TF)) * IDF\n",
                "    \"\"\"\n",
                "    vec = {}\n",
                "    for term, freq in tf_doc.items():\n",
                "        if freq <= 0:\n",
                "            continue\n",
                "        idf_val = idf.get(term, 0.0)\n",
                "        if idf_val == 0:\n",
                "            continue\n",
                "        \n",
                "        # Log-normalization reduces impact of very frequent words within a doc\n",
                "        weight = (1 + math.log(freq)) * idf_val\n",
                "        vec[term] = weight\n",
                "    return vec\n",
                "\n",
                "print(\"‚úÖ TF-IDF vectorization function defined.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def cosine_similarity(v1: dict, v2: dict) -> float:\n",
                "    \"\"\"\n",
                "    Calculate cosine similarity between two sparse vectors.\n",
                "    Result ranges from 0.0 (no match) to 1.0 (perfect match).\n",
                "    \"\"\"\n",
                "    if not v1 or not v2:\n",
                "        return 0.0\n",
                "\n",
                "    # Optimization: iterate over the shorter vector\n",
                "    if len(v1) > len(v2):\n",
                "        v1, v2 = v2, v1\n",
                "\n",
                "    dot_product = 0.0\n",
                "    for term, val in v1.items():\n",
                "        dot_product += val * v2.get(term, 0.0)\n",
                "\n",
                "    norm1 = math.sqrt(sum(v ** 2 for v in v1.values()))\n",
                "    norm2 = math.sqrt(sum(v ** 2 for v in v2.values()))\n",
                "\n",
                "    if norm1 == 0.0 or norm2 == 0.0:\n",
                "        return 0.0\n",
                "\n",
                "    return dot_product / (norm1 * norm2)\n",
                "\n",
                "print(\"‚úÖ Cosine similarity function defined.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üèóÔ∏è Search Engine Class\n",
                "This class encapsulates everything. When initialized, it pre-computes vectors for all documents so that searching is fast."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "MAX_TEXT_DISPLAY_LENGTH = 200\n",
                "\n",
                "class SimpleSearchEngine:\n",
                "    def __init__(self, docs: list):\n",
                "        self.docs = docs or []\n",
                "        print(\"‚öôÔ∏è Building Index...\")\n",
                "        \n",
                "        # 1. Build TF for all docs\n",
                "        self.tf_docs = build_tf(self.docs)\n",
                "        \n",
                "        # 2. Compute IDF global stats\n",
                "        self.idf = compute_idf(self.tf_docs)\n",
                "        \n",
                "        # 3. Pre-compute TF-IDF vectors for all docs\n",
                "        self.doc_vectors = [tfidf_vector(tf_doc, self.idf) for tf_doc in self.tf_docs]\n",
                "        print(\"‚úÖ Index built successfully.\")\n",
                "\n",
                "    def ranked_search(self, query: str, top_k: int = 5) -> list:\n",
                "        query = (query or \"\").strip()\n",
                "        if not query:\n",
                "            return []\n",
                "\n",
                "        # 1. Convert Query to Vector\n",
                "        q_tf = defaultdict(int)\n",
                "        for token in preprocess(query):\n",
                "            q_tf[token] += 1\n",
                "            \n",
                "        q_vec = tfidf_vector(q_tf, self.idf)\n",
                "        if not q_vec:\n",
                "            return []\n",
                "\n",
                "        # 2. Compare Query vs All Docs\n",
                "        results = []\n",
                "        for i, d_vec in enumerate(self.doc_vectors):\n",
                "            score = cosine_similarity(q_vec, d_vec)\n",
                "            if score > 0:\n",
                "                # Truncate text for display\n",
                "                full_text = self.docs[i]\n",
                "                preview = full_text[:MAX_TEXT_DISPLAY_LENGTH] + (\"...\" if len(full_text) > MAX_TEXT_DISPLAY_LENGTH else \"\")\n",
                "                \n",
                "                results.append({\n",
                "                    \"score\": round(score, 3),\n",
                "                    \"text\": preview,\n",
                "                    \"index\": i\n",
                "                })\n",
                "\n",
                "        # 3. Sort by Score (Descending)\n",
                "        results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
                "        return results[:top_k]\n",
                "\n",
                "print(\"‚úÖ SimpleSearchEngine class defined.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## üß™ Step 6: Testing & Execution\n",
                "\n",
                "We'll load some **sample data** directly in code so you can test it immediately without needing PDF files, but we also check for local PDFs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "SAMPLE_DOCUMENTS = [\n",
                "    \"Information retrieval is the process of obtaining information system resources relevant to an information need.\",\n",
                "    \"Search engines use algorithms like TF-IDF and PageRank to rank web pages.\",\n",
                "    \"Machine learning improves search results by learning from user feedback.\",\n",
                "    \"Natural language processing enables computers to understand human language.\",\n",
                "    \"Deep learning models are used in modern neural search engines.\",\n",
                "    \"Football is a popular sport played with a spherical ball.\",\n",
                "    \"Artificial Intelligence is simulating human intelligence in machines.\",\n",
                "    \"PyTorch and TensorFlow are popular deep learning libraries.\",\n",
                "]\n",
                "\n",
                "print(f\"‚úÖ Loaded {len(SAMPLE_DOCUMENTS)} sample documents.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize Engine\n",
                "\n",
                "# 1. Try to load real PDFs\n",
                "pdf_folder = \"pdfs\"\n",
                "documents = load_all_pdfs_from_folder(pdf_folder)\n",
                "\n",
                "# 2. Fallback to sample data if no PDFs found\n",
                "if not documents:\n",
                "    print(\"\\n‚ö†Ô∏è No PDFs found. Using Sample Documents instead.\")\n",
                "    documents = SAMPLE_DOCUMENTS\n",
                "\n",
                "# 3. Create Engine Instance\n",
                "engine = SimpleSearchEngine(documents)\n",
                "print(f\"\\nüéâ Search engine ready with {len(documents)} documents!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üîç Try a Search Query"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "query = \"machine learning AI\"\n",
                "results = engine.ranked_search(query)\n",
                "\n",
                "print(f\"\\nüîé Query: '{query}'\")\n",
                "print(\"=\" * 80)\n",
                "if results:\n",
                "    for i, res in enumerate(results, 1):\n",
                "        print(f\"\\n{i}. Score: {res['score']:.3f}\")\n",
                "        print(f\"   {res['text']}\")\n",
                "else:\n",
                "    print(\"No results found.\")\n",
                "print(\"\\n\" + \"=\" * 80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üåê Step 7: Flask Web Application (Optional)\n",
                "\n",
                "This section shows how to wrap the search engine in a **Flask web interface**. \n",
                "\n",
                "**‚ö†Ô∏è Note:** \n",
                "- Running this cell will start a web server that blocks the notebook\n",
                "- You'll need to stop the cell manually (interrupt kernel) to continue\n",
                "- The Flask app will reuse the `engine` we already initialized above\n",
                "- Make sure you have `templates/index.html` file in the correct location"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Flask Web Application Code\n",
                "# This reuses the search engine we already created above\n",
                "\n",
                "from flask import Flask, render_template, request\n",
                "\n",
                "# Configuration\n",
                "FLASK_HOST = \"127.0.0.1\"\n",
                "FLASK_PORT = 5000\n",
                "FLASK_DEBUG = True\n",
                "TOP_K_RESULTS = 10\n",
                "\n",
                "# Create Flask app\n",
                "app = Flask(__name__)\n",
                "\n",
                "# Note: We're reusing the 'engine' variable initialized above\n",
                "# No need to create a new search engine instance\n",
                "\n",
                "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
                "def index():\n",
                "    \"\"\"\n",
                "    Main route that handles displaying the search form and processing queries.\n",
                "    \"\"\"\n",
                "    results = []\n",
                "    query = \"\"\n",
                "\n",
                "    if request.method == \"POST\":\n",
                "        query = request.form.get(\"query\", \"\").strip()\n",
                "        if query:\n",
                "            # Use the global 'engine' variable\n",
                "            results = engine.ranked_search(query, top_k=TOP_K_RESULTS)\n",
                "\n",
                "    return render_template(\"index.html\", query=query, results=results)\n",
                "\n",
                "\n",
                "# Run the Flask app\n",
                "if __name__ == \"__main__\":\n",
                "    print(\"=\" * 50)\n",
                "    print(\"üîç Smart Document Search Engine\")\n",
                "    print(\"=\" * 50)\n",
                "    print(f\"Server running on: http://{FLASK_HOST}:{FLASK_PORT}\")\n",
                "    print(\"Press INTERRUPT (‚ñ† button) to stop the server\")\n",
                "    print(\"=\" * 50)\n",
                "    \n",
                "    # Run without reloader in notebook environment to avoid issues\n",
                "    app.run(debug=FLASK_DEBUG, host=FLASK_HOST, port=FLASK_PORT, use_reloader=False)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}