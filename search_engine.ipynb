{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTN0XmjswR9m",
        "outputId": "36c42a54-d8f8-4994-bb89-c308a26acc89"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Muhammad\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Muhammad\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Muhammad\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\Muhammad\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution ~t-dlp (d:\\Python\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~t-dlp (d:\\Python\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~t-dlp (d:\\Python\\Lib\\site-packages)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in d:\\python\\lib\\site-packages (3.9.1)\n",
            "Requirement already satisfied: requests in d:\\python\\lib\\site-packages (2.32.3)\n",
            "Requirement already satisfied: scikit-learn in d:\\python\\lib\\site-packages (1.6.0)\n",
            "Requirement already satisfied: numpy in d:\\python\\lib\\site-packages (2.2.0)\n",
            "Requirement already satisfied: Flask in d:\\python\\lib\\site-packages (3.1.0)\n",
            "Requirement already satisfied: click in d:\\python\\lib\\site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in d:\\python\\lib\\site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in d:\\python\\lib\\site-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in d:\\python\\lib\\site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\python\\lib\\site-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in d:\\python\\lib\\site-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\python\\lib\\site-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\python\\lib\\site-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: scipy>=1.6.0 in d:\\python\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\python\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in d:\\python\\lib\\site-packages (from Flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in d:\\python\\lib\\site-packages (from Flask) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in d:\\python\\lib\\site-packages (from Flask) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.9 in d:\\python\\lib\\site-packages (from Flask) (1.9.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\muhammad\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in d:\\python\\lib\\site-packages (from Jinja2>=3.1.2->Flask) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk requests scikit-learn numpy Flask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PpuU8JYPwR_-"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import requests\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iiVKZ1JxwSCW"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words and t.isalpha()]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JSdECbrzwSEv"
      },
      "outputs": [],
      "source": [
        "# ------------------------------ 2. Load Gutenberg Books ------------------------------\n",
        "book_links = [\n",
        "    \"https://www.gutenberg.org/files/1342/1342-0.txt\",  # Pride and Prejudice\n",
        "    \"https://www.gutenberg.org/files/11/11-0.txt\",      # Alice in Wonderland\n",
        "    \"https://www.gutenberg.org/files/98/98-0.txt\"       # A Tale of Two Cities\n",
        "]\n",
        "\n",
        "def load_gutenberg_book(url):\n",
        "    r = requests.get(url)\n",
        "    r.encoding = \"utf-8\"\n",
        "    return r.text\n",
        "\n",
        "def clean_gutenberg_text(text):\n",
        "    start = \"*** START OF THIS PROJECT GUTENBERG EBOOK\"\n",
        "    end = \"*** END OF THIS PROJECT GUTENBERG EBOOK\"\n",
        "    if start in text and end in text:\n",
        "        text = text.split(start)[1].split(end)[0]\n",
        "    return text\n",
        "\n",
        "docs = []\n",
        "for link in book_links:\n",
        "    raw_text = load_gutenberg_book(link)\n",
        "    clean_text = clean_gutenberg_text(raw_text)\n",
        "    docs.append(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6avR6g7wSG-",
        "outputId": "13a15de6-443f-4e82-ef09-d0a7605bf9a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index built for Gutenberg books!\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------ 3. Build Positional Index ------------------------------\n",
        "def build_index(docs):\n",
        "    index = {}  # term -> {doc_id: [positions]}\n",
        "    for doc_id, doc in enumerate(docs):\n",
        "        tokens = preprocess(doc)\n",
        "        for pos, token in enumerate(tokens):\n",
        "            if token not in index:\n",
        "                index[token] = {}\n",
        "            if doc_id not in index[token]:\n",
        "                index[token][doc_id] = []\n",
        "            index[token][doc_id].append(pos)\n",
        "    return index\n",
        "\n",
        "index = build_index(docs)\n",
        "print(\"Index built for Gutenberg books!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vkiFiIlLwSJF"
      },
      "outputs": [],
      "source": [
        "# ------------------------------ 4. Boolean + Phrase Search ------------------------------\n",
        "def boolean_search(query, index):\n",
        "    q_tokens = preprocess(query)\n",
        "    if not q_tokens:\n",
        "        return []\n",
        "\n",
        "    if q_tokens[0] not in index:\n",
        "        return []\n",
        "    result_docs = set(index[q_tokens[0]].keys())\n",
        "    for term in q_tokens[1:]:\n",
        "        if term not in index:\n",
        "            return []\n",
        "        result_docs &= set(index[term].keys())\n",
        "\n",
        "    # Phrase check\n",
        "    if len(q_tokens) > 1:\n",
        "        valid_docs = []\n",
        "        for doc_id in result_docs:\n",
        "            pos_lists = [sorted(index[t][doc_id]) for t in q_tokens]\n",
        "            for i in range(len(pos_lists[0])):\n",
        "                if all(pos_lists[j][0] + j == pos_lists[0][i] + j for j in range(1, len(pos_lists)) if i + j < len(pos_lists[j])):\n",
        "                    valid_docs.append(doc_id)\n",
        "                    break\n",
        "        result_docs = valid_docs\n",
        "\n",
        "    return list(result_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kAfgKaZNwSLf"
      },
      "outputs": [],
      "source": [
        "# ------------------------------ 5. Edit Distance Search ------------------------------\n",
        "def edit_distance(s1, s2):\n",
        "    m, n = len(s1), len(s2)\n",
        "    dp = [[0]*(n+1) for _ in range(m+1)]\n",
        "    for i in range(m+1):\n",
        "        dp[i][0] = i\n",
        "    for j in range(n+1):\n",
        "        dp[0][j] = j\n",
        "    for i in range(1,m+1):\n",
        "        for j in range(1,n+1):\n",
        "            if s1[i-1]==s2[j-1]:\n",
        "                dp[i][j] = dp[i-1][j-1]\n",
        "            else:\n",
        "                dp[i][j] = 1 + min(dp[i-1][j-1], dp[i-1][j], dp[i][j-1])\n",
        "    return dp[m][n]\n",
        "\n",
        "def fuzzy_search(query, index, max_dist=2):\n",
        "    q_tokens = preprocess(query)\n",
        "    matched_docs = set()\n",
        "    for term in index.keys():\n",
        "        for qt in q_tokens:\n",
        "            if edit_distance(term, qt) <= max_dist:\n",
        "                matched_docs.update(index[term].keys())\n",
        "    return list(matched_docs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Qu6pKNgbwSOO"
      },
      "outputs": [],
      "source": [
        "# ------------------------------ 6. Soundex Search ------------------------------\n",
        "def soundex(word):\n",
        "    word = word.upper()\n",
        "    codes = {\"B\":\"1\",\"F\":\"1\",\"P\":\"1\",\"V\":\"1\",\n",
        "             \"C\":\"2\",\"G\":\"2\",\"J\":\"2\",\"K\":\"2\",\"Q\":\"2\",\"S\":\"2\",\"X\":\"2\",\"Z\":\"2\",\n",
        "             \"D\":\"3\",\"T\":\"3\",\n",
        "             \"L\":\"4\",\n",
        "             \"M\":\"5\",\"N\":\"5\",\n",
        "             \"R\":\"6\"}\n",
        "    sound = word[0]\n",
        "    for char in word[1:]:\n",
        "        code = codes.get(char,\"0\")\n",
        "        if code != sound[-1]:\n",
        "            sound += code\n",
        "    sound = sound.replace(\"0\",\"\")\n",
        "    return (sound+\"000\")[:4]\n",
        "\n",
        "def soundex_search(query, index):\n",
        "    q_tokens = preprocess(query)\n",
        "    matched_docs = set()\n",
        "    query_sdx = [soundex(q) for q in q_tokens]\n",
        "    for term in index.keys():\n",
        "        if soundex(term) in query_sdx:\n",
        "            matched_docs.update(index[term].keys())\n",
        "    return list(matched_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IcpfMZK3wSRm"
      },
      "outputs": [],
      "source": [
        "# ------------------------------ 7. TF-IDF Ranking ------------------------------\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = vectorizer.fit_transform(docs)\n",
        "\n",
        "def rank_documents(query, docs_list, doc_ids):\n",
        "    if not doc_ids:\n",
        "        return []\n",
        "\n",
        "    query_vec = np.zeros((1, len(vectorizer.get_feature_names_out())))\n",
        "    q_tokens = preprocess(query)\n",
        "    for token in q_tokens:\n",
        "        if token in vectorizer.vocabulary_:\n",
        "            idx = vectorizer.vocabulary_[token]\n",
        "            query_vec[0, idx] = 1\n",
        "\n",
        "    tfidf_dense = tfidf_matrix.toarray()\n",
        "    scores = np.dot(tfidf_dense, query_vec.T).flatten()\n",
        "\n",
        "    ranked_idx = np.argsort(scores)[::-1]\n",
        "    ranked_docs = [(i, docs_list[i], scores[i]) for i in ranked_idx if i in doc_ids and scores[i] > 0]\n",
        "    return ranked_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87adb1a4",
        "outputId": "d0021e21-556e-4b59-dcf7-dc84211530cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SearchEngine class defined.\n"
          ]
        }
      ],
      "source": [
        "class SearchEngine:\n",
        "    def __init__(self, docs, index, vectorizer, tfidf_matrix, book_links):\n",
        "        self.docs = docs\n",
        "        self.index = index\n",
        "        self.vectorizer = vectorizer\n",
        "        self.tfidf_matrix = tfidf_matrix\n",
        "        self.book_links = book_links\n",
        "\n",
        "    def boolean_search(self, query):\n",
        "        return boolean_search(query, self.index)\n",
        "\n",
        "    def fuzzy_search(self, query, max_dist=2):\n",
        "        return fuzzy_search(query, self.index, max_dist)\n",
        "\n",
        "    def soundex_search(self, query):\n",
        "        return soundex_search(query, self.index)\n",
        "\n",
        "    def ranked_search(self, query, search_type='tfidf'):\n",
        "        doc_ids = []\n",
        "        if search_type == 'boolean':\n",
        "            doc_ids = self.boolean_search(query)\n",
        "        elif search_type == 'fuzzy':\n",
        "            doc_ids = self.fuzzy_search(query)\n",
        "        elif search_type == 'soundex':\n",
        "            doc_ids = self.soundex_search(query)\n",
        "        elif search_type == 'tfidf':\n",
        "            doc_ids = list(range(len(self.docs)))\n",
        "        else:\n",
        "            print(\"Invalid search type. Using TF-IDF for ranking all documents.\")\n",
        "            doc_ids = list(range(len(self.docs)))\n",
        "\n",
        "        if not doc_ids:\n",
        "            return []\n",
        "\n",
        "        return rank_documents(query, self.docs, doc_ids)\n",
        "\n",
        "print(\"SearchEngine class defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d0de980",
        "outputId": "2d9a6e51-37f8-42b3-c9d7-f21565f92fd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SearchEngine instance initialized.\n"
          ]
        }
      ],
      "source": [
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = vectorizer.fit_transform(docs)\n",
        "\n",
        "search_engine_instance = SearchEngine(docs, index, vectorizer, tfidf_matrix, book_links)\n",
        "print(\"SearchEngine instance initialized.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e8c368b",
        "outputId": "91348c3d-6b30-4c9c-82af-ef23d0399da8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Flask app is running. Open a browser to http://127.0.0.1:5000/\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
            " * Running on http://127.0.0.1:5000\n",
            "Press CTRL+C to quit\n",
            "127.0.0.1 - - [23/Dec/2025 22:25:12] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [23/Dec/2025 22:25:13] \"GET /static/style.css HTTP/1.1\" 304 -\n",
            "127.0.0.1 - - [23/Dec/2025 22:25:30] \"POST / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [23/Dec/2025 22:25:30] \"GET /static/style.css HTTP/1.1\" 304 -\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask, request, render_template\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Ensure search_engine_instance is accessible\n",
        "# This assumes search_engine_instance has already been created in a previous cell\n",
        "# and is available in the global scope.\n",
        "\n",
        "@app.route('/', methods=['GET', 'POST'])\n",
        "def search():\n",
        "    query = None\n",
        "    results = []\n",
        "    if request.method == 'POST':\n",
        "        query = request.form['query']\n",
        "        if query:\n",
        "            # Use the search_engine_instance to get ranked results\n",
        "            # search_engine_instance is defined in the previous cell\n",
        "            ranked_results = search_engine_instance.ranked_search(query, search_type='tfidf') # Defaulting to tfidf for this example\n",
        "            results = [\n",
        "                {\n",
        "                    \"doc_id\": doc_id,\n",
        "                    \"score\": f\"{score:.4f}\",\n",
        "                    \"link\": search_engine_instance.book_links[doc_id]\n",
        "                }\n",
        "                for doc_id, _, score in ranked_results\n",
        "            ]\n",
        "        return render_template('index.html', query=query, results=results)\n",
        "    else:\n",
        "        return render_template('index.html', query=None, results=[])\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(\"Flask app is running. Open a browser to http://127.0.0.1:5000/\")\n",
        "    app.run(debug=True, port=5000, use_reloader=False) # use_reloader=False to avoid running twice in some environments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
