{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# **Search Engine**\n",
                "\n",
                "##  Overview\n",
                "implements  **TF-IDF (Term Frequency-Inverse Document Frequency)** search engine from scratch. \n",
                "It supports both **Arabic** and **English** documents and uses **Cosine Similarity** to rank search results.\n",
                "\n",
                "###  Features\n",
                "- **Bilingual Support**: Handles English and Arabic text.\n",
                "- **PDF Processing**: Extracts text from PDF files automatically.\n",
                "- **Vector Space Model**: Represents text as mathematical vectors.\n",
                "- **Ranked Retrieval**: Returns the most relevant results first."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "'pip' is not recognized as an internal or external command,\n",
                        "operable program or batch file.\n"
                    ]
                }
            ],
            "source": [
                "# Install required packages\n",
                "!pip install flask nltk PyPDF2"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## **Step 2: Imports & Initialization**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "import math\n",
                "import os\n",
                "import glob\n",
                "from collections import defaultdict\n",
                "\n",
                "# PDF Handling\n",
                "import PyPDF2\n",
                "\n",
                "# NLP Libraries\n",
                "import nltk\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.stem import WordNetLemmatizer"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## **Step 3: Text Preprocessing**\n",
                "\n",
                "The quality of a search engine depends heavily on preprocessing. We perform:\n",
                "1.  **Normalization**: Unifying character forms (especially for Arabic).\n",
                "2.  **Tokenization**: Splitting text into words.\n",
                "3.  **Stopword Removal**: Removing common words like \"the\", \"in\", \"في\", \"من\".\n",
                "4.  **Lemmatization**: Converting words to their base form (e.g., \"running\" -> \"run\")."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==========================================\n",
                "# Configuration & Constants\n",
                "# ==========================================\n",
                "\n",
                "# Define Arabic Stopwords manually as they might be incomplete in NLTK\n",
                "ARABIC_STOPWORDS = {\n",
                "    \"في\", \"على\", \"من\", \"إلى\", \"عن\", \"مع\", \"هذا\", \"هذه\",\n",
                "    \"هو\", \"هي\", \"هم\", \"هن\", \"كان\", \"كانت\", \"يكون\",\n",
                "    \"ما\", \"لا\", \"لم\", \"لن\", \"أن\", \"إن\", \"كل\", \"أي\"\n",
                "}\n",
                "\n",
                "# Initialize English Stopwords and Lemmatizer\n",
                "STOP_WORDS_EN = set(stopwords.words(\"english\"))\n",
                "LEMMATIZER = WordNetLemmatizer()\n",
                "\n",
                "# Regex to identify Arabic characters\n",
                "ARABIC_REGEX = re.compile(r\"[\\u0600-\\u06FF]+\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "def normalize_arabic(text: str) -> str:\n",
                "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
                "    text = re.sub(\"ى\", \"ي\", text)\n",
                "    text = re.sub(\"ؤ\", \"و\", text)\n",
                "    text = re.sub(\"ئ\", \"ي\", text)\n",
                "    text = re.sub(\"ة\", \"ه\", text)\n",
                "    text = re.sub(\"[ًٌٍَُِّْ]\", \"\", text)  # Remove diacritics\n",
                "    return text\n",
                "\n",
                "def is_arabic_token(token: str) -> bool:\n",
                "    return ARABIC_REGEX.search(token) is not None\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "def preprocess(text: str) -> list:\n",
                "    if not text:\n",
                "        return []\n",
                "\n",
                "    # 1. Lowercase & Normalize\n",
                "    text = text.lower()\n",
                "    text = normalize_arabic(text)\n",
                "\n",
                "    # 2. Remove Punctuation (keep only word chars and spaces)\n",
                "    text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
                "\n",
                "    tokens = text.split()\n",
                "    clean_tokens = []\n",
                "\n",
                "    for token in tokens:\n",
                "        # Handle Arabic Tokens\n",
                "        if is_arabic_token(token):\n",
                "            if token not in ARABIC_STOPWORDS and len(token) > 1:\n",
                "                clean_tokens.append(token)\n",
                "        \n",
                "        # Handle English Tokens\n",
                "        elif token.isalpha():\n",
                "            if token not in STOP_WORDS_EN:\n",
                "                # Lemmatize: 'running' -> 'run'\n",
                "                clean_tokens.append(LEMMATIZER.lemmatize(token))\n",
                "\n",
                "    return clean_tokens"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## **Step 4: Document Loading (PDFs)**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "MIN_PARAGRAPH_LENGTH = 20\n",
                "\n",
                "def load_pdf(filename: str) -> list:\n",
                "    if not os.path.exists(filename):\n",
                "        return []\n",
                "\n",
                "    docs = []\n",
                "    try:\n",
                "        with open(filename, \"rb\") as f:\n",
                "            reader = PyPDF2.PdfReader(f)\n",
                "            for page in reader.pages:\n",
                "                text = page.extract_text()\n",
                "                if not text:\n",
                "                    continue\n",
                "                # Split by newlines to get potential paragraphs\n",
                "                for paragraph in text.split(\"\\n\"):\n",
                "                    paragraph = paragraph.strip()\n",
                "                    if len(paragraph) > MIN_PARAGRAPH_LENGTH:\n",
                "                        docs.append(paragraph)\n",
                "    except Exception as e:\n",
                "        print(f\"Error loading PDF {filename}: {e}\")\n",
                "        return []\n",
                "    \n",
                "    return docs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_all_pdfs_from_folder(folder_path: str) -> list:\n",
                "    all_documents = []\n",
                "    \n",
                "    if not os.path.exists(folder_path):\n",
                "        print(f\"Folder not found: {folder_path}\")\n",
                "        return []\n",
                "    \n",
                "    pdf_files = glob.glob(os.path.join(folder_path, \"*.pdf\"))\n",
                "    \n",
                "    if not pdf_files:\n",
                "        print(f\"ℹ No PDF files found in: {folder_path}\")\n",
                "        return []\n",
                "    \n",
                "    print(f\" Found {len(pdf_files)} PDF file(s). Loading...\")\n",
                "    \n",
                "    for pdf_file in pdf_files:\n",
                "        print(f\"   → Processing: {os.path.basename(pdf_file)}\")\n",
                "        docs = load_pdf(pdf_file)\n",
                "        all_documents.extend(docs)\n",
                "    \n",
                "    print(f\"Total loaded documents (paragraphs): {len(all_documents)}\")\n",
                "    return all_documents\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## **Step 5: The Search Engine Core**\n",
                "\n",
                "Here we implement the **Vector Space Model** logic:\n",
                "\n",
                "### 1. TF (Term Frequency)\n",
                "How often a word appears in a specific document.\n",
                "\n",
                "### 2. IDF (Inverse Document Frequency)\n",
                "How unique a word is across all documents. Common words like \"is\" have low IDF, while specific terms like \"algorithm\" have high IDF.\n",
                "\n",
                "$$ IDF(t) = \\log \\left( \\frac{Total\\ Documents}{Documents\\ with\\ term\\ t} \\right) $$\n",
                "\n",
                "### 3. Cosine Similarity\n",
                "Measures the angle between two vectors (Query Vector vs. Document Vector). A value of **1.0** means identical direction (perfect match), **0.0** means no similarity."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_tf(docs: list) -> list:\n",
                "    tf_docs = []\n",
                "    for doc in docs:\n",
                "        freq = defaultdict(int)\n",
                "        for token in preprocess(doc):\n",
                "            freq[token] += 1\n",
                "        tf_docs.append(freq)\n",
                "    return tf_docs\n",
                "\n",
                "def compute_idf(tf_docs: list) -> dict:\n",
                "    N = len(tf_docs)\n",
                "    df = defaultdict(int)\n",
                "\n",
                "    for doc in tf_docs:\n",
                "        for term in doc.keys():\n",
                "            df[term] += 1\n",
                "\n",
                "    # Standard IDF formula\n",
                "    idf = {term: math.log(N / df_val) for term, df_val in df.items() if df_val > 0}\n",
                "    return idf"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "def tfidf_vector(tf_doc: dict, idf: dict) -> dict:\n",
                "    vec = {}\n",
                "    for term, freq in tf_doc.items():\n",
                "        if freq <= 0:\n",
                "            continue\n",
                "        idf_val = idf.get(term, 0.0)\n",
                "        if idf_val == 0:\n",
                "            continue\n",
                "        \n",
                "        # Log-normalization reduces impact of very frequent words within a doc\n",
                "        weight = (1 + math.log(freq)) * idf_val\n",
                "        vec[term] = weight\n",
                "    return vec"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "def cosine_similarity(v1: dict, v2: dict) -> float:\n",
                "    if not v1 or not v2:\n",
                "        return 0.0\n",
                "\n",
                "    # Optimization: iterate over the shorter vector\n",
                "    if len(v1) > len(v2):\n",
                "        v1, v2 = v2, v1\n",
                "\n",
                "    dot_product = 0.0\n",
                "    for term, val in v1.items():\n",
                "        dot_product += val * v2.get(term, 0.0)\n",
                "\n",
                "    norm1 = math.sqrt(sum(v ** 2 for v in v1.values()))\n",
                "    norm2 = math.sqrt(sum(v ** 2 for v in v2.values()))\n",
                "\n",
                "    if norm1 == 0.0 or norm2 == 0.0:\n",
                "        return 0.0\n",
                "\n",
                "    return dot_product / (norm1 * norm2)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **Search Engine Class**\n",
                "This class encapsulates everything. When initialized, it pre-computes vectors for all documents so that searching is fast."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "MAX_TEXT_DISPLAY_LENGTH = 200\n",
                "\n",
                "class SimpleSearchEngine:\n",
                "    def __init__(self, docs: list):\n",
                "        self.docs = docs or []      \n",
                "\n",
                "        # 1. Build TF for all docs\n",
                "        self.tf_docs = build_tf(self.docs)\n",
                "        \n",
                "        # 2. Compute IDF global stats\n",
                "        self.idf = compute_idf(self.tf_docs)\n",
                "        \n",
                "        # 3. Pre-compute TF-IDF vectors for all docs\n",
                "        self.doc_vectors = [tfidf_vector(tf_doc, self.idf) for tf_doc in self.tf_docs]\n",
                "\n",
                "    def ranked_search(self, query: str, top_k: int = 5) -> list:\n",
                "        query = (query or \"\").strip()\n",
                "        if not query:\n",
                "            return []\n",
                "\n",
                "        # 1. Convert Query to Vector\n",
                "        q_tf = defaultdict(int)\n",
                "        for token in preprocess(query):\n",
                "            q_tf[token] += 1\n",
                "            \n",
                "        q_vec = tfidf_vector(q_tf, self.idf)\n",
                "        if not q_vec:\n",
                "            return []\n",
                "\n",
                "        # 2. Compare Query vs All Docs\n",
                "        results = []\n",
                "        for i, d_vec in enumerate(self.doc_vectors):\n",
                "            score = cosine_similarity(q_vec, d_vec)\n",
                "            if score > 0:\n",
                "                # Truncate text for display\n",
                "                full_text = self.docs[i]\n",
                "                preview = full_text[:MAX_TEXT_DISPLAY_LENGTH] + (\"...\" if len(full_text) > MAX_TEXT_DISPLAY_LENGTH else \"\")\n",
                "                \n",
                "                results.append({\n",
                "                    \"score\": round(score, 3),\n",
                "                    \"text\": preview,\n",
                "                    \"index\": i\n",
                "                })\n",
                "\n",
                "        # 3. Sort by Score (Descending)\n",
                "        results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
                "        return results[:top_k]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## **Step 6: Testing & Execution**\n",
                "\n",
                "We'll load some **sample data** directly in code so you can test it immediately without needing PDF files, but we also check for local PDFs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "SAMPLE_DOCUMENTS = [\n",
                "    \"Information retrieval is the process of obtaining information system resources relevant to an information need.\",\n",
                "    \"Search engines use algorithms like TF-IDF and PageRank to rank web pages.\",\n",
                "    \"Machine learning improves search results by learning from user feedback.\",\n",
                "    \"Natural language processing enables computers to understand human language.\",\n",
                "    \"Deep learning models are used in modern neural search engines.\",\n",
                "    \"Football is a popular sport played with a spherical ball.\",\n",
                "    \"Artificial Intelligence is simulating human intelligence in machines.\",\n",
                "    \"PyTorch and TensorFlow are popular deep learning libraries.\",\n",
                "]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## **Initialize Engine Class**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ℹ No PDF files found in: pdfs\n",
                        "\n",
                        "No PDFs found. Using Sample Documents instead.\n",
                        "\n",
                        "Search engine ready with 8 documents!\n"
                    ]
                }
            ],
            "source": [
                "# Initialize Engine\n",
                "\n",
                "# 1. Try to load real PDFs\n",
                "pdf_folder = \"pdfs\"\n",
                "documents = load_all_pdfs_from_folder(pdf_folder)\n",
                "\n",
                "# 2. Fallback to sample data if no PDFs found\n",
                "if not documents:\n",
                "    print(\"\\nNo PDFs found. Using Sample Documents instead.\")\n",
                "    documents = SAMPLE_DOCUMENTS\n",
                "\n",
                "# 3. Create Engine Instance\n",
                "engine = SimpleSearchEngine(documents)\n",
                "print(f\"\\nSearch engine ready with {len(documents)} documents!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Flask Web Application"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        " * Serving Flask app '__main__'\n",
                        " * Debug mode: on\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
                        " * Running on http://127.0.0.1:5000\n",
                        "Press CTRL+C to quit\n",
                        "127.0.0.1 - - [23/Dec/2025 01:16:35] \"GET / HTTP/1.1\" 200 -\n",
                        "127.0.0.1 - - [23/Dec/2025 01:16:35] \"GET /static/style.css HTTP/1.1\" 304 -\n",
                        "127.0.0.1 - - [23/Dec/2025 01:16:40] \"POST / HTTP/1.1\" 200 -\n",
                        "127.0.0.1 - - [23/Dec/2025 01:16:40] \"GET /static/style.css HTTP/1.1\" 304 -\n",
                        "127.0.0.1 - - [23/Dec/2025 01:16:44] \"POST / HTTP/1.1\" 200 -\n",
                        "127.0.0.1 - - [23/Dec/2025 01:16:44] \"GET /static/style.css HTTP/1.1\" 304 -\n",
                        "127.0.0.1 - - [23/Dec/2025 01:16:48] \"POST / HTTP/1.1\" 200 -\n",
                        "127.0.0.1 - - [23/Dec/2025 01:16:48] \"GET /static/style.css HTTP/1.1\" 304 -\n",
                        "127.0.0.1 - - [23/Dec/2025 01:16:54] \"POST / HTTP/1.1\" 200 -\n",
                        "127.0.0.1 - - [23/Dec/2025 01:16:54] \"GET /static/style.css HTTP/1.1\" 304 -\n",
                        "127.0.0.1 - - [23/Dec/2025 01:16:58] \"POST / HTTP/1.1\" 200 -\n",
                        "127.0.0.1 - - [23/Dec/2025 01:16:58] \"GET /static/style.css HTTP/1.1\" 304 -\n"
                    ]
                }
            ],
            "source": [
                "# Flask Web Application Code\n",
                "# This reuses the search engine we already created above\n",
                "\n",
                "from flask import Flask, render_template, request\n",
                "\n",
                "# Configuration\n",
                "FLASK_HOST = \"127.0.0.1\"\n",
                "FLASK_PORT = 5000\n",
                "FLASK_DEBUG = True\n",
                "TOP_K_RESULTS = 10\n",
                "\n",
                "# Create Flask app\n",
                "app = Flask(__name__)\n",
                "\n",
                "# Note: We're reusing the 'engine' variable initialized above\n",
                "# No need to create a new search engine instance\n",
                "\n",
                "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
                "def index():\n",
                "    \"\"\"\n",
                "    Main route that handles displaying the search form and processing queries.\n",
                "    \"\"\"\n",
                "    results = []\n",
                "    query = \"\"\n",
                "\n",
                "    if request.method == \"POST\":\n",
                "        query = request.form.get(\"query\", \"\").strip()\n",
                "        if query:\n",
                "            # Use the global 'engine' variable\n",
                "            results = engine.ranked_search(query, top_k=TOP_K_RESULTS)\n",
                "\n",
                "    return render_template(\"index.html\", query=query, results=results)\n",
                "\n",
                "\n",
                "# Run the Flask app\n",
                "if __name__ == \"__main__\":\n",
                "    \n",
                "    # Run without reloader in notebook environment to avoid issues\n",
                "    app.run(debug=FLASK_DEBUG, host=FLASK_HOST, port=FLASK_PORT, use_reloader=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
